{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1412,"status":"ok","timestamp":1660902117487,"user":{"displayName":"Edoardo Filippi-Mazzola","userId":"12937409144207641672"},"user_tz":-120},"id":"6gIAUN82RfGv","outputId":"629d2744-35d8-41aa-c46c-bac9b4bc6e0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Aug 19 09:41:55 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3492,"status":"ok","timestamp":1660902122661,"user":{"displayName":"Edoardo Filippi-Mazzola","userId":"12937409144207641672"},"user_tz":-120},"id":"xnP0Isv3RpQn","outputId":"8e848f15-f57d-46d7-e8b9-c3a96071a9a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3065,"status":"ok","timestamp":1660902125724,"user":{"displayName":"Edoardo Filippi-Mazzola","userId":"12937409144207641672"},"user_tz":-120},"id":"r1ZEar9kRNRE"},"outputs":[],"source":["import numpy as np \n","import torch\n","\n","from CIFAR10_loader import *\n","from model import *\n","from trainer import trainer"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6107,"status":"ok","timestamp":1660902131828,"user":{"displayName":"Edoardo Filippi-Mazzola","userId":"12937409144207641672"},"user_tz":-120},"id":"NgPQrdfBRNRI","outputId":"8ed8da44-03cd-47bf-9b5f-76e357fdedd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading or checking the presence of the data:\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Train size: 49000\n","Validation size: 1000\n","Downloading or checking the presence of the data:\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Test size: 10000\n"]}],"source":["train, validation = train_val_load('./data')\n","test = test_loader('./data')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1660902131828,"user":{"displayName":"Edoardo Filippi-Mazzola","userId":"12937409144207641672"},"user_tz":-120},"id":"SPCD_up2RNRK","outputId":"57e93154-dcb1-46e1-c0b2-404f3fcba491"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing normalization: \n","\n","Training mean tensor(-0.0987)\n","Training std tensor(1.0038)\n","Test mean tensor(-0.0486)\n","Test std tensor(1.0157)\n"]}],"source":["print('Testing normalization: \\n')\n","data_train = next(iter(train))\n","print('Training mean',data_train[0].mean())\n","print('Training std',data_train[0].std())\n","\n","data_test = next(iter(test))\n","print('Test mean',data_test[0].mean())\n","print('Test std',data_test[0].std())"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38811,"status":"ok","timestamp":1660902293753,"user":{"displayName":"Edoardo Filippi-Mazzola","userId":"12937409144207641672"},"user_tz":-120},"id":"LJNbU2hwVbKH","outputId":"cd25c37b-520e-4b5d-da60-b63389c70257"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1 | Batch: 0 | Loss: 2.3057000637054443 | Training accuracy: 9.375%\n","Epoch: 1 | Batch: 100 | Loss: 2.3026756508515613 | Training accuracy: 10.426980198019802%\n","Epoch: 1 | Batch: 200 | Loss: 2.301958932212336 | Training accuracy: 10.572139303482587%\n","Epoch: 1 | Batch: 300 | Loss: 2.3010435595464864 | Training accuracy: 11.762873754152825%\n","Epoch: 1 | Batch: 400 | Loss: 2.299762848309448 | Training accuracy: 12.515586034912719%\n","Epoch: 1 | Batch: 500 | Loss: 2.297188130205501 | Training accuracy: 13.360778443113773%\n","Epoch: 1 | Batch: 600 | Loss: 2.2910877837119203 | Training accuracy: 13.768718801996672%\n","Epoch: 1 | Batch: 700 | Loss: 2.2789505242281054 | Training accuracy: 14.595221112696148%\n","Epoch: 1 | Batch: 800 | Loss: 2.2614459300904386 | Training accuracy: 15.644506866416979%\n","Epoch: 1 | Batch: 900 | Loss: 2.2387281730357604 | Training accuracy: 16.835460599334073%\n","Epoch: 1 | Batch: 1000 | Loss: 2.2177011920260146 | Training accuracy: 17.794705294705295%\n","Epoch: 1 | Batch: 1100 | Loss: 2.196869303268481 | Training accuracy: 18.752838328792006%\n","Epoch: 1 | Batch: 1200 | Loss: 2.1739977570397966 | Training accuracy: 19.75176935886761%\n","Epoch: 1 | Batch: 1300 | Loss: 2.1523217016875424 | Training accuracy: 20.806110684089163%\n","Epoch: 1 | Batch: 1400 | Loss: 2.1299468643915476 | Training accuracy: 21.741167023554603%\n","Epoch: 1 | Batch: 1500 | Loss: 2.1083460739658006 | Training accuracy: 22.639073950699533%\n","Epoch: 1 | Loss 2.1012975205484943 | Train accuracy: 22.93469387755102% | Validation accuracy 34.4%\n","Epoch: 2 | Batch: 0 | Loss: 2.1327366828918457 | Training accuracy: 21.875%\n","Epoch: 2 | Batch: 100 | Loss: 1.7937044896701775 | Training accuracy: 34.62252475247525%\n","Epoch: 2 | Batch: 200 | Loss: 1.7791823996833307 | Training accuracy: 35.26119402985075%\n","Epoch: 2 | Batch: 300 | Loss: 1.75358224192331 | Training accuracy: 36.21262458471761%\n","Epoch: 2 | Batch: 400 | Loss: 1.7388711537506218 | Training accuracy: 36.89993765586035%\n","Epoch: 2 | Batch: 500 | Loss: 1.724026372332773 | Training accuracy: 37.4189121756487%\n","Epoch: 2 | Batch: 600 | Loss: 1.7148206795313197 | Training accuracy: 37.63519134775375%\n","Epoch: 2 | Batch: 700 | Loss: 1.7024836769797833 | Training accuracy: 38.021576319543506%\n","Epoch: 2 | Batch: 800 | Loss: 1.690125325943498 | Training accuracy: 38.45193508114856%\n","Epoch: 2 | Batch: 900 | Loss: 1.687903207097281 | Training accuracy: 38.630688124306324%\n","Epoch: 2 | Batch: 1000 | Loss: 1.6778987659202826 | Training accuracy: 38.92982017982018%\n","Epoch: 2 | Batch: 1100 | Loss: 1.6667482771730553 | Training accuracy: 39.39032697547684%\n","Epoch: 2 | Batch: 1200 | Loss: 1.6572478056748046 | Training accuracy: 39.75593255620316%\n","Epoch: 2 | Batch: 1300 | Loss: 1.6459981432151647 | Training accuracy: 40.14700230591853%\n","Epoch: 2 | Batch: 1400 | Loss: 1.63843401315636 | Training accuracy: 40.48224482512491%\n","Epoch: 2 | Batch: 1500 | Loss: 1.630709884962505 | Training accuracy: 40.77490006662225%\n","Epoch: 2 | Loss 1.6288928443240123 | Train accuracy: 40.857142857142854% | Validation accuracy 46.2%\n","Epoch: 3 | Batch: 0 | Loss: 1.6041196584701538 | Training accuracy: 40.625%\n","Epoch: 3 | Batch: 100 | Loss: 1.5221636814646202 | Training accuracy: 44.306930693069305%\n","Epoch: 3 | Batch: 200 | Loss: 1.5034746115480488 | Training accuracy: 45.42910447761194%\n","Epoch: 3 | Batch: 300 | Loss: 1.4910817724525731 | Training accuracy: 45.63953488372093%\n","Epoch: 3 | Batch: 400 | Loss: 1.4776527180041459 | Training accuracy: 46.29831670822943%\n","Epoch: 3 | Batch: 500 | Loss: 1.4712457685413476 | Training accuracy: 46.60678642714571%\n","Epoch: 3 | Batch: 600 | Loss: 1.4682507340404238 | Training accuracy: 46.71900998336106%\n","Epoch: 3 | Batch: 700 | Loss: 1.4650583515493745 | Training accuracy: 46.794757489301%\n","Epoch: 3 | Batch: 800 | Loss: 1.4607307683216053 | Training accuracy: 46.88670411985019%\n","Epoch: 3 | Batch: 900 | Loss: 1.4562608544755062 | Training accuracy: 47.13859600443951%\n","Epoch: 3 | Batch: 1000 | Loss: 1.4524908040786957 | Training accuracy: 47.277722277722276%\n","Epoch: 3 | Batch: 1100 | Loss: 1.4490173923005634 | Training accuracy: 47.38589918256131%\n","Epoch: 3 | Batch: 1200 | Loss: 1.4454723293338587 | Training accuracy: 47.57493755203997%\n","Epoch: 3 | Batch: 1300 | Loss: 1.4452883272240657 | Training accuracy: 47.713297463489624%\n","Epoch: 3 | Batch: 1400 | Loss: 1.442753552922515 | Training accuracy: 47.887669521770164%\n","Epoch: 3 | Batch: 1500 | Loss: 1.4376122042943762 | Training accuracy: 48.1283311125916%\n","Epoch: 3 | Loss 1.4347428686858468 | Train accuracy: 48.24285714285714% | Validation accuracy 52.7%\n","Epoch: 4 | Batch: 0 | Loss: 1.3499802350997925 | Training accuracy: 46.875%\n","Epoch: 4 | Batch: 100 | Loss: 1.347662893852385 | Training accuracy: 51.42326732673267%\n","Epoch: 4 | Batch: 200 | Loss: 1.3520887129342378 | Training accuracy: 51.243781094527364%\n","Epoch: 4 | Batch: 300 | Loss: 1.3498912257609574 | Training accuracy: 51.692275747508305%\n","Epoch: 4 | Batch: 400 | Loss: 1.3409229790480655 | Training accuracy: 52.06514962593516%\n","Epoch: 4 | Batch: 500 | Loss: 1.3316702601200567 | Training accuracy: 52.40144710578842%\n","Epoch: 4 | Batch: 600 | Loss: 1.3303515116704283 | Training accuracy: 52.45424292845258%\n","Epoch: 4 | Batch: 700 | Loss: 1.3280782682579355 | Training accuracy: 52.48305991440799%\n","Epoch: 4 | Batch: 800 | Loss: 1.3260599877801578 | Training accuracy: 52.563202247191015%\n","Epoch: 4 | Batch: 900 | Loss: 1.323027010829282 | Training accuracy: 52.68104883462819%\n","Epoch: 4 | Batch: 1000 | Loss: 1.319529958478697 | Training accuracy: 52.75661838161838%\n","Epoch: 4 | Batch: 1100 | Loss: 1.3166740367782863 | Training accuracy: 52.87522706630336%\n","Epoch: 4 | Batch: 1200 | Loss: 1.3121740329176261 | Training accuracy: 53.054746044962535%\n","Epoch: 4 | Batch: 1300 | Loss: 1.308906492593195 | Training accuracy: 53.20666794773251%\n","Epoch: 4 | Batch: 1400 | Loss: 1.3040024641579513 | Training accuracy: 53.40827980014276%\n","Epoch: 4 | Batch: 1500 | Loss: 1.3020281814321686 | Training accuracy: 53.443537641572284%\n","Epoch: 4 | Loss 1.3018162557101436 | Train accuracy: 53.45510204081633% | Validation accuracy 57.8%\n","Epoch: 5 | Batch: 0 | Loss: 1.3316948413848877 | Training accuracy: 50.0%\n","Epoch: 5 | Batch: 100 | Loss: 1.2190962953142601 | Training accuracy: 56.59034653465346%\n","Epoch: 5 | Batch: 200 | Loss: 1.2276208332521998 | Training accuracy: 56.54539800995025%\n","Epoch: 5 | Batch: 300 | Loss: 1.2199921687180022 | Training accuracy: 56.748338870431894%\n","Epoch: 5 | Batch: 400 | Loss: 1.2175579824649783 | Training accuracy: 56.67861596009975%\n","Epoch: 5 | Batch: 500 | Loss: 1.2155251215080063 | Training accuracy: 57.12949101796407%\n","Epoch: 5 | Batch: 600 | Loss: 1.2131157073125665 | Training accuracy: 57.2119384359401%\n","Epoch: 5 | Batch: 700 | Loss: 1.2048068912155108 | Training accuracy: 57.40905848787447%\n","Epoch: 5 | Batch: 800 | Loss: 1.2033816981553733 | Training accuracy: 57.572565543071164%\n","Epoch: 5 | Batch: 900 | Loss: 1.2004417762507609 | Training accuracy: 57.65122086570477%\n","Epoch: 5 | Batch: 1000 | Loss: 1.1984118990131192 | Training accuracy: 57.66108891108891%\n","Epoch: 5 | Batch: 1100 | Loss: 1.1898794419975522 | Training accuracy: 57.8905540417802%\n","Epoch: 5 | Batch: 1200 | Loss: 1.1874590101190452 | Training accuracy: 58.02196086594505%\n","Epoch: 5 | Batch: 1300 | Loss: 1.1845953190537437 | Training accuracy: 58.22204073789393%\n","Epoch: 5 | Batch: 1400 | Loss: 1.180551089362363 | Training accuracy: 58.3489471805853%\n","Epoch: 5 | Batch: 1500 | Loss: 1.1754557545545656 | Training accuracy: 58.49850099933378%\n","Epoch: 5 | Loss 1.1743531528247555 | Train accuracy: 58.5265306122449% | Validation accuracy 60.2%\n","Epoch: 6 | Batch: 0 | Loss: 0.99785977602005 | Training accuracy: 68.75%\n","Epoch: 6 | Batch: 100 | Loss: 1.1133292435419442 | Training accuracy: 60.82920792079208%\n","Epoch: 6 | Batch: 200 | Loss: 1.1054504504844325 | Training accuracy: 61.11629353233831%\n","Epoch: 6 | Batch: 300 | Loss: 1.1119463332863742 | Training accuracy: 61.02574750830565%\n","Epoch: 6 | Batch: 400 | Loss: 1.1054613576862877 | Training accuracy: 61.20635910224439%\n","Epoch: 6 | Batch: 500 | Loss: 1.0964975195254631 | Training accuracy: 61.62674650698603%\n","Epoch: 6 | Batch: 600 | Loss: 1.0927469886083174 | Training accuracy: 61.714850249584025%\n","Epoch: 6 | Batch: 700 | Loss: 1.0882213360572848 | Training accuracy: 61.77781740370899%\n","Epoch: 6 | Batch: 800 | Loss: 1.0814299330431574 | Training accuracy: 61.96551186017478%\n","Epoch: 6 | Batch: 900 | Loss: 1.0812896937958805 | Training accuracy: 61.910377358490564%\n","Epoch: 6 | Batch: 1000 | Loss: 1.074407111871969 | Training accuracy: 62.109765234765234%\n","Epoch: 6 | Batch: 1100 | Loss: 1.072264148147403 | Training accuracy: 62.131017257039055%\n","Epoch: 6 | Batch: 1200 | Loss: 1.0711760010747091 | Training accuracy: 62.15653621981682%\n","Epoch: 6 | Batch: 1300 | Loss: 1.066943512248773 | Training accuracy: 62.34627209838586%\n","Epoch: 6 | Batch: 1400 | Loss: 1.0622624723422875 | Training accuracy: 62.566916488222695%\n","Epoch: 6 | Batch: 1500 | Loss: 1.060204650623492 | Training accuracy: 62.6478181212525%\n","Epoch: 6 | Loss 1.0589425435810114 | Train accuracy: 62.689795918367345% | Validation accuracy 65.3%\n","Epoch: 7 | Batch: 0 | Loss: 0.9396147131919861 | Training accuracy: 68.75%\n","Epoch: 7 | Batch: 100 | Loss: 0.9979608147451193 | Training accuracy: 65.93440594059406%\n","Epoch: 7 | Batch: 200 | Loss: 0.9733316569185969 | Training accuracy: 66.34017412935323%\n","Epoch: 7 | Batch: 300 | Loss: 0.9718667641034554 | Training accuracy: 66.05066445182725%\n","Epoch: 7 | Batch: 400 | Loss: 0.9768192180968877 | Training accuracy: 65.87437655860349%\n","Epoch: 7 | Batch: 500 | Loss: 0.9720405366368399 | Training accuracy: 66.10528942115768%\n","Epoch: 7 | Batch: 600 | Loss: 0.9720691072563958 | Training accuracy: 66.18656405990016%\n","Epoch: 7 | Batch: 700 | Loss: 0.9654482769728047 | Training accuracy: 66.37393009985735%\n","Epoch: 7 | Batch: 800 | Loss: 0.9637104552634498 | Training accuracy: 66.38966916354556%\n","Epoch: 7 | Batch: 900 | Loss: 0.96316010170057 | Training accuracy: 66.32907880133186%\n","Epoch: 7 | Batch: 1000 | Loss: 0.9596419566042059 | Training accuracy: 66.49288211788212%\n","Epoch: 7 | Batch: 1100 | Loss: 0.957205275423845 | Training accuracy: 66.57016348773843%\n","Epoch: 7 | Batch: 1200 | Loss: 0.9575647277498524 | Training accuracy: 66.67360532889259%\n","Epoch: 7 | Batch: 1300 | Loss: 0.9562915191569757 | Training accuracy: 66.72031129900077%\n","Epoch: 7 | Batch: 1400 | Loss: 0.9528724323375832 | Training accuracy: 66.87633832976445%\n","Epoch: 7 | Batch: 1500 | Loss: 0.9510138341182871 | Training accuracy: 66.93037974683544%\n","Epoch: 7 | Loss 0.9505865463972403 | Train accuracy: 66.95102040816326% | Validation accuracy 66.6%\n","Epoch: 8 | Batch: 0 | Loss: 0.7285754084587097 | Training accuracy: 81.25%\n","Epoch: 8 | Batch: 100 | Loss: 0.8594084724341289 | Training accuracy: 69.77103960396039%\n","Epoch: 8 | Batch: 200 | Loss: 0.8534831329068141 | Training accuracy: 70.80223880597015%\n","Epoch: 8 | Batch: 300 | Loss: 0.862934800378508 | Training accuracy: 70.27616279069767%\n","Epoch: 8 | Batch: 400 | Loss: 0.8734265140761759 | Training accuracy: 69.84102244389027%\n","Epoch: 8 | Batch: 500 | Loss: 0.8746507472977667 | Training accuracy: 70.01621756487026%\n","Epoch: 8 | Batch: 600 | Loss: 0.8711032237169548 | Training accuracy: 70.02391846921797%\n","Epoch: 8 | Batch: 700 | Loss: 0.8708840521017938 | Training accuracy: 69.87785306704707%\n","Epoch: 8 | Batch: 800 | Loss: 0.8680427449844899 | Training accuracy: 69.854088639201%\n","Epoch: 8 | Batch: 900 | Loss: 0.8626586252060635 | Training accuracy: 70.06104328523863%\n","Epoch: 8 | Batch: 1000 | Loss: 0.8598987159135935 | Training accuracy: 70.2765984015984%\n","Epoch: 8 | Batch: 1100 | Loss: 0.8575680037177118 | Training accuracy: 70.41042234332426%\n","Epoch: 8 | Batch: 1200 | Loss: 0.857801974043064 | Training accuracy: 70.34242298084929%\n","Epoch: 8 | Batch: 1300 | Loss: 0.8594289098126809 | Training accuracy: 70.24404304381245%\n","Epoch: 8 | Batch: 1400 | Loss: 0.8578022128126606 | Training accuracy: 70.25785153461813%\n","Epoch: 8 | Batch: 1500 | Loss: 0.8550676943102652 | Training accuracy: 70.34477015323118%\n","Epoch: 8 | Loss 0.8543744072434797 | Train accuracy: 70.39387755102041% | Validation accuracy 68.9%\n","Epoch: 9 | Batch: 0 | Loss: 0.5920339226722717 | Training accuracy: 81.25%\n","Epoch: 9 | Batch: 100 | Loss: 0.7546164679645312 | Training accuracy: 73.63861386138613%\n","Epoch: 9 | Batch: 200 | Loss: 0.7625103753004501 | Training accuracy: 74.19154228855722%\n","Epoch: 9 | Batch: 300 | Loss: 0.75551222913289 | Training accuracy: 74.16943521594685%\n","Epoch: 9 | Batch: 400 | Loss: 0.7738542900121123 | Training accuracy: 73.31670822942644%\n","Epoch: 9 | Batch: 500 | Loss: 0.774785482062551 | Training accuracy: 73.32210578842316%\n","Epoch: 9 | Batch: 600 | Loss: 0.7691867522610206 | Training accuracy: 73.45049916805324%\n","Epoch: 9 | Batch: 700 | Loss: 0.7698039954977267 | Training accuracy: 73.43972895863052%\n","Epoch: 9 | Batch: 800 | Loss: 0.7707471805863018 | Training accuracy: 73.3458177278402%\n","Epoch: 9 | Batch: 900 | Loss: 0.771937242060999 | Training accuracy: 73.30396781354051%\n","Epoch: 9 | Batch: 1000 | Loss: 0.7723671812396664 | Training accuracy: 73.36101398601399%\n","Epoch: 9 | Batch: 1100 | Loss: 0.7702468879703604 | Training accuracy: 73.39350590372389%\n","Epoch: 9 | Batch: 1200 | Loss: 0.7718149003240091 | Training accuracy: 73.3451290591174%\n","Epoch: 9 | Batch: 1300 | Loss: 0.7703163879683712 | Training accuracy: 73.3858570330515%\n","Epoch: 9 | Batch: 1400 | Loss: 0.7690468821714471 | Training accuracy: 73.43861527480371%\n","Epoch: 9 | Batch: 1500 | Loss: 0.7682511861883426 | Training accuracy: 73.50099933377749%\n","Epoch: 9 | Loss 0.7671130191019248 | Train accuracy: 73.5265306122449% | Validation accuracy 70.1%\n","Epoch: 10 | Batch: 0 | Loss: 0.6596513986587524 | Training accuracy: 78.125%\n","Epoch: 10 | Batch: 100 | Loss: 0.6680626653798736 | Training accuracy: 76.39232673267327%\n","Epoch: 10 | Batch: 200 | Loss: 0.6739055422408071 | Training accuracy: 76.39925373134328%\n","Epoch: 10 | Batch: 300 | Loss: 0.6820492312757676 | Training accuracy: 76.34966777408638%\n","Epoch: 10 | Batch: 400 | Loss: 0.6880226834605162 | Training accuracy: 76.0676433915212%\n","Epoch: 10 | Batch: 500 | Loss: 0.6894583669370282 | Training accuracy: 76.07909181636727%\n","Epoch: 10 | Batch: 600 | Loss: 0.6890777168972123 | Training accuracy: 76.19072379367721%\n","Epoch: 10 | Batch: 700 | Loss: 0.6874089068846083 | Training accuracy: 76.1055634807418%\n","Epoch: 10 | Batch: 800 | Loss: 0.6859948348426045 | Training accuracy: 76.22503121098627%\n","Epoch: 10 | Batch: 900 | Loss: 0.6842295589973606 | Training accuracy: 76.31451165371809%\n","Epoch: 10 | Batch: 1000 | Loss: 0.6815618962853343 | Training accuracy: 76.38611388611389%\n","Epoch: 10 | Batch: 1100 | Loss: 0.6842227027591633 | Training accuracy: 76.25454132606721%\n","Epoch: 10 | Batch: 1200 | Loss: 0.6852570768448832 | Training accuracy: 76.2099292256453%\n","Epoch: 10 | Batch: 1300 | Loss: 0.6879084673090956 | Training accuracy: 76.1457532667179%\n","Epoch: 10 | Batch: 1400 | Loss: 0.6879526236966029 | Training accuracy: 76.14204139900072%\n","Epoch: 10 | Batch: 1500 | Loss: 0.68890027665123 | Training accuracy: 76.11592271818787%\n","Epoch: 10 | Loss 0.6894516987682323 | Train accuracy: 76.08979591836734% | Validation accuracy 72.2%\n","SAVING MODEL\n","Finished training\n"]}],"source":["training = trainer().fit(train_set = train,validation_set = validation)"]},{"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3158,"status":"ok","timestamp":1660902341240,"user":{"displayName":"Edoardo Filippi-Mazzola","userId":"12937409144207641672"},"user_tz":-120},"id":"HZnzISpjWkzS","outputId":"7947287f-beb6-4d84-be3b-a8a6acf95cee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the network on 10000 test images: 70 %\n"]}],"source":["trainer().prediction(test_set = test)"]},{"cell_type":"markdown","metadata":{},"source":["The classifier performance are non-optimal. These could be improved by 10% if dropout (at 0.5) and batch normalization are added."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"test.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3d46e833fbb150be313e23f92b66840c9ee391f86f43a5370bfb6957e259579a"}}},"nbformat":4,"nbformat_minor":0}
